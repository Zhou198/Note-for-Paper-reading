### Content
* [Consistency of Plug-in Confidence Sets for Classification in Semi-supervised Learning](#CPCSCSL)


<h2 id="#CPCSCSL">

### [Consistency of Plug-in Confidence Sets for Classification in Semi-supervised Learning](https://arxiv.org/pdf/1507.07235.pdf) 
<p align="right"> Jan. 4, 2020 </p>

Not assigining label for an instance is better than giving a non-confident prediction when wrong classification may lead to huge cost. Therefore, this paper proposed a method, level-$\varepsilon$-confidence sets, to give confident predictions in the binary classification cases. Many works are related to this method, like Conformal Prediction (CP) and Classification with Reject Option (CRO), even if they have different focuses.


The most difference from others is that level-$\varepsilon$-confidence sets control the proportion ($\varepsilon$) of classifying or the probability ($1-\varepsilon$) of reject option. Generally, the smaller $\varepsilon$, the more accurate prediction. However, CP does not take into account the reject option, although it guarantees the coverage probability. Similarly, (CRO), though, considers the reject option, devotes to minimize a risk given a preset torelance (d) of reject option and do not care how much the reject probability is (it does not control any part in a risk function).  Both two methods can compromise the rejection probability to improve the missclassification. Therefore, it is much easier when comparing two classifiers when controling the probability of classifying.

This method takes estimation of $\eta^\ast(X)$ and unlabeled data to produce level-$\varepsilon$-confidence sets. Explicitly, we use labeled $D_n$ to obtain an estimation $\hat \eta(X)$ of $\eta^\ast(X)$ if we do not know the distribution of data (most time we do not know) and take $\hat f(X)=\max{\hat \eta(X), 1- \hat \eta(X)}$ as our score. Secondly, we use another (un)labled $D_N$ to estimate the distribution of $\hat f$, say $\hat F_{\hat f}$ and get the quantile threshold $\alpha_\varepsilon$. Given an upcoming instance $x$, we assigin a label for it based on $\hat \eta(x)$ if $\hat f(x)\geq \alpha_\varepsilon$, otherwise it falls into reject option.

Based on above procedure, if the estimation $\hat \eta$ is consistent ($\hat \eta \rightarrow \eta^\ast$ with probability 1 as $n\rightarrow \infty$), then asymtocically the plug-in $\varepsilon$-confidence set performs as well as the $\varepsilon$-confidence set (which is obtained by knew distribution and no estimation in any steps). However, the key this method relies on continuous cdfs of score functions $f^\ast$ and $\hat f$, which transduces to the continuity of $\eta^\ast$ and $\hat \eta$. 

In numerical studyies, for data generated based on continuous $\eta^\ast$, it uses randomforest, logistic regression a as well as kernel estimation to get $\hat\eta$. From the results, these three plug-in $\varepsilon$-confidence sets match the theories, although randomforest is outperformed by the others. Moreover, increasing the sample size $n$ on estimation of $\eta^\ast$ can improve the convergence rate. However, if $\eta^\ast$ is continuose but we use CART to estimate it (now $\hat\eta$ is not continuous), we cannot control the classidying probability and the risk also convergence slowly. For the case data generated by discrete $\eta^\ast$ but estimated by kernel method ($\hat\eta$ is continuous), it is irrelavant to compare the performance of $\varepsilon$-confidence sets and its plug-in counterpart, although the results match the theoretical study.

In conclusion, this method outputs a optimal result when control the classifying probability, which is helpful when compare two classifiers with reject option. For further thinking, I think this method is sort of like a dual problem for the topic "minimizing the size of prediction set when controling the accuracy". Moreover, can we use hard classifiers with this proposed method since sometimes probability estimation is difficulty, especially in high-dimensional cases. Also, we need to read related work tackling multiclass cases.

</h2>













































